{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e226516",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af63c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Optional\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "WEEK_H = 168      # 1주일 = 168시간\n",
    "EPS    = 1e-3     # 0 나눔 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723707e",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4cf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 컬럼명 매핑(dict)을 파일 내부에 정의 ──\n",
    "TRAIN_COL_RENAMES = {\n",
    "    '건물번호': 'building_number',\n",
    "    '일시': 'date_time',\n",
    "    '기온(°C)': 'temperature',\n",
    "    '강수량(mm)': 'rainfall',\n",
    "    '풍속(m/s)': 'windspeed',\n",
    "    '습도(%)': 'humidity',\n",
    "    '일조(hr)': 'sunshine',\n",
    "    '일사(MJ/m2)': 'solar_radiation',\n",
    "    '전력소비량(kWh)': 'power_consumption'\n",
    "}\n",
    "TEST_COL_RENAMES = TRAIN_COL_RENAMES.copy()\n",
    "\n",
    "BUILDING_INFO_RENAMES = {\n",
    "    '건물번호': 'building_number',\n",
    "    '건물유형': 'building_type',\n",
    "    '연면적(m2)': 'total_area',\n",
    "    '냉방면적(m2)': 'cooling_area',\n",
    "    '태양광용량(kW)': 'solar_power_capacity',\n",
    "    'ESS저장용량(kWh)': 'ess_capacity',\n",
    "    'PCS용량(kW)': 'pcs_capacity'\n",
    "}\n",
    "TYPE_TRANSLATION = {\n",
    "    '건물기타': 'Other Buildings',\n",
    "    '공공': 'Public',\n",
    "    '학교': 'School',\n",
    "    '백화점': 'Department Store',\n",
    "    '병원': 'Hospital',\n",
    "    '상용': 'Commercial',\n",
    "    '아파트': 'Apartment',\n",
    "    '연구소': 'Research Institute',\n",
    "    '호텔': 'Hotel',\n",
    "    'IDC(전화국)': 'IDC'\n",
    "}\n",
    "\n",
    "def load_raw(data_dir: str = \"../data/raw\"):\n",
    "    train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "    test  = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "    info  = pd.read_csv(os.path.join(data_dir, 'building_info.csv'))\n",
    "    return train, test, info\n",
    "\n",
    "def rename_columns(df: pd.DataFrame, mapping: dict):\n",
    "    df = df.rename(columns=mapping)\n",
    "    if 'num_date_time' in df.columns:\n",
    "        df = df.drop('num_date_time', axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_building_info(info: pd.DataFrame) -> pd.DataFrame:\n",
    "    info = info.rename(columns=BUILDING_INFO_RENAMES)\n",
    "    info['building_type'] = info['building_type'].replace(TYPE_TRANSLATION)\n",
    "    return info\n",
    "\n",
    "def merge_datasets(train: pd.DataFrame, test: pd.DataFrame, info: pd.DataFrame):\n",
    "    train = train.merge(info, on='building_number', how='left')\n",
    "    test  = test.merge(info, on='building_number', how='left')\n",
    "    return train, test\n",
    "\n",
    "def save_processed(df: pd.DataFrame, name: str, out_dir: str = \"../data/processed\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    path = os.path.join(out_dir, f\"{name}.pkl\")\n",
    "    df.to_pickle(path)\n",
    "    print(f\"Saved processed data to: {path}\")\n",
    "\n",
    "def load_and_process(data_dir: str = \"../data/raw\") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    1) raw CSV 로드\n",
    "    2) 컬럼명 리네임\n",
    "    3) building_info 전처리 & 병합\n",
    "    4) processed/train.pkl, processed/test.pkl 저장\n",
    "    5) train_df, test_df 반환\n",
    "    \"\"\"\n",
    "    train, test, info = load_raw(data_dir)\n",
    "    train = rename_columns(train, TRAIN_COL_RENAMES)\n",
    "    test  = rename_columns(test, TEST_COL_RENAMES)\n",
    "    info  = preprocess_building_info(info)\n",
    "    train, test = merge_datasets(train, test, info)\n",
    "\n",
    "    save_processed(train, \"train\")\n",
    "    save_processed(test,  \"test\")\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf491232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    date_time 컬럼을 datetime 타입으로 변환하고\n",
    "    시간, 일, 월, 요일, 주말 여부, 연중 일(day_of_year) 피처 및\n",
    "    하루를 4분할하는 시간대(time_of_day) 피처 추가\n",
    "    \"\"\"\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], format='%Y%m%d %H')\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day_of_week'] = df['date_time'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['day_of_year'] = df['date_time'].dt.dayofyear\n",
    "    conditions = [\n",
    "        (df['hour'] >= 0) & (df['hour'] < 6),\n",
    "        (df['hour'] >= 6) & (df['hour'] < 12),\n",
    "        (df['hour'] >= 12) & (df['hour'] < 18),\n",
    "        (df['hour'] >= 18) & (df['hour'] < 24)\n",
    "    ]\n",
    "    choices = [0, 1, 3, 2]  # 0: 새벽, 1: 오전, 2: 오후, 3: 저녁\n",
    "    df['time_of_day'] = np.select(conditions, choices, default=np.nan)\n",
    "    # -------------------\n",
    "    \n",
    "    return df\n",
    "\n",
    "KR_HOLIDAYS_2024 = {\"2024-06-06\", \"2024-08-15\"}\n",
    "def _ensure_dt(df):\n",
    "    if not np.issubdtype(df[\"date_time\"].dtype, np.datetime64):\n",
    "        df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "    return df\n",
    "\n",
    "def _nth_weekday_in_month(series_dt, weekday_target):\n",
    "    # 월 내 해당 요일의 n번째 (1=첫째, 2=둘째, ...)\n",
    "    first_of_month = series_dt.values.astype(\"datetime64[M]\").astype(\"datetime64[ns]\")\n",
    "    first_weekday = pd.to_datetime(first_of_month).weekday\n",
    "    weekday = series_dt.dt.weekday.values\n",
    "    day = series_dt.dt.day.values\n",
    "    first_occ_day = 1 + ((weekday_target - first_weekday) % 7)\n",
    "    nth = ((day - first_occ_day) // 7) + 1\n",
    "    nth = np.where(day >= first_occ_day, nth, 0)\n",
    "    return nth\n",
    "\n",
    "def add_holiday(df: pd.DataFrame, kr_holidays: set[str] = None) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    _ensure_dt(df)\n",
    "    if kr_holidays is None:\n",
    "        kr_holidays = KR_HOLIDAYS_2024\n",
    "\n",
    "    # 기본 파생 (is_weekend은 '계산만' 하고 이후 어떤 건물에도 수정하지 않음)\n",
    "    df[\"weekday\"] = df[\"date_time\"].dt.weekday          # 0=Mon..6=Sun\n",
    "    df[\"date\"]    = df[\"date_time\"].dt.date\n",
    "    df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int) # 그대로 유지\n",
    "    df[\"holiday\"] = 0\n",
    "\n",
    "    # 공휴일 여부는 컬럼으로 저장하지 않고, 로컬 불리언으로만 사용\n",
    "    is_kr = df[\"date\"].astype(str).isin(kr_holidays).values\n",
    "\n",
    "    bt = df[\"building_type\"]\n",
    "\n",
    "    # ── Apartment: 항상 영업\n",
    "    mm = bt == \"Apartment\"\n",
    "    df.loc[mm, \"holiday\"] = 0\n",
    "\n",
    "    # ── Hospital: 주말 or 공휴일 휴식\n",
    "    mm = bt == \"Hospital\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = (df.loc[mm, \"is_weekend\"].values | is_kr[mm]).astype(int)\n",
    "\n",
    "    # ── Public: 기본 주말 or 공휴일 휴식, 단 33/92는 항상 영업\n",
    "    mm = bt == \"Public\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = (df.loc[mm, \"is_weekend\"].values | is_kr[mm]).astype(int)\n",
    "        mm_always_open = df[\"building_number\"].isin([33, 92])\n",
    "        df.loc[mm_always_open, \"holiday\"] = 0\n",
    "\n",
    "    # ── Hotel: 항상 영업\n",
    "    mm = bt == \"Hotel\"\n",
    "    df.loc[mm, \"holiday\"] = 0\n",
    "\n",
    "    # ── School: 주말 or 공휴일 휴식\n",
    "    mm = bt == \"School\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = (df.loc[mm, \"is_weekend\"].values | is_kr[mm]).astype(int)\n",
    "\n",
    "    # ── IDC(전화국): 개별 규칙\n",
    "    mm_idc = bt == \"IDC\"\n",
    "    if mm_idc.any():\n",
    "        # 36,43,52: 주말 or 공휴일\n",
    "        ids = [36, 43, 52]\n",
    "        mmx = df[\"building_number\"].isin(ids)\n",
    "        df.loc[mmx, \"holiday\"] = (df.loc[mmx, \"is_weekend\"].values | is_kr[mmx]).astype(int)\n",
    "        # 64: 주말만\n",
    "        mmx = df[\"building_number\"].eq(64)\n",
    "        df.loc[mmx, \"holiday\"] = df.loc[mmx, \"is_weekend\"].astype(int)\n",
    "        # 67: 주말 + 8/15\n",
    "        mmx = df[\"building_number\"].eq(67)\n",
    "        if mmx.any():\n",
    "            df.loc[mmx, \"holiday\"] = df.loc[mmx, \"is_weekend\"].astype(int)\n",
    "            df.loc[mmx & (df[\"date\"].astype(str) == \"2024-08-15\"), \"holiday\"] = 1\n",
    "        # 30,35,57: 휴일 없음 → holiday=0 유지\n",
    "\n",
    "    # ── Commercial: 개별 규칙\n",
    "    mm = bt == \"Commercial\"\n",
    "    if mm.any():\n",
    "        # 2: 주말만\n",
    "        mmx = df[\"building_number\"].eq(2)\n",
    "        df.loc[mmx, \"holiday\"] = df.loc[mmx, \"is_weekend\"].astype(int)\n",
    "        # 6,16,20,51,86: 주말 or 공휴일\n",
    "        ids = [6, 16, 20, 51, 86]\n",
    "        mmx = df[\"building_number\"].isin(ids)\n",
    "        df.loc[mmx, \"holiday\"] = (df.loc[mmx, \"is_weekend\"].values | is_kr[mmx]).astype(int)\n",
    "        # 41,56,76,99: 휴일 없음 → holiday=0 유지\n",
    "\n",
    "    # ── Other Buildings: 개별 규칙\n",
    "    # 26: 월/화\n",
    "    mmx = df[\"building_number\"].eq(26)\n",
    "    df.loc[mmx, \"holiday\"] = df.loc[mmx, \"weekday\"].isin([0, 1]).astype(int)\n",
    "    # 82: 월\n",
    "    mmx = df[\"building_number\"].eq(82)\n",
    "    df.loc[mmx, \"holiday\"] = df.loc[mmx, \"weekday\"].eq(0).astype(int)\n",
    "    # 47,69: 주말 or 공휴일\n",
    "    mmx = df[\"building_number\"].isin([47, 69])\n",
    "    df.loc[mmx, \"holiday\"] = (df.loc[mmx, \"is_weekend\"].values | is_kr[mmx]).astype(int)\n",
    "    # 58,61,78: 주말에도 영업 → holiday=0 유지 (is_weekend는 건드리지 않음)\n",
    "    # 97: 토요일만\n",
    "    mmx = df[\"building_number\"].eq(97)\n",
    "    df.loc[mmx, \"holiday\"] = df.loc[mmx, \"weekday\"].eq(5).astype(int)\n",
    "\n",
    "    # ── Department Store: 공휴일에도 영업. 개별 규칙만 휴일 처리.\n",
    "    mm = bt == \"Department Store\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = 0\n",
    "\n",
    "        nth_sun = _nth_weekday_in_month(df[\"date_time\"], 6)  # Sun\n",
    "        nth_mon = _nth_weekday_in_month(df[\"date_time\"], 0)  # Mon\n",
    "\n",
    "        def mark_nth_weekday(building, weekday, nth_set):\n",
    "            if weekday == 6:\n",
    "                nth = nth_sun\n",
    "            elif weekday == 0:\n",
    "                nth = nth_mon\n",
    "            else:\n",
    "                nth = _nth_weekday_in_month(df[\"date_time\"], weekday)\n",
    "            sel = df[\"building_number\"].eq(building) & df[\"weekday\"].eq(weekday) & pd.Series(nth).isin(list(nth_set)).values\n",
    "            df.loc[sel, \"holiday\"] = 1\n",
    "\n",
    "        # 매주/격주/특정일\n",
    "        df.loc[df[\"building_number\"].eq(18) & df[\"weekday\"].eq(6), \"holiday\"] = 1  # 18: 매주 일요일\n",
    "\n",
    "        special = {\n",
    "            19: [\"2024-06-10\", \"2024-07-08\", \"2024-08-19\"],\n",
    "            45: [\"2024-06-10\", \"2024-07-08\", \"2024-08-19\"],\n",
    "            54: [\"2024-06-17\", \"2024-07-01\", \"2024-08-19\"],\n",
    "            74: [\"2024-06-17\", \"2024-07-01\"],\n",
    "            79: [\"2024-06-17\", \"2024-07-01\", \"2024-08-19\"],\n",
    "            95: [\"2024-07-08\", \"2024-08-05\"],\n",
    "            29: [\"2024-06-10\", \"2024-07-10\", \"2024-08-10\"],\n",
    "        }\n",
    "        for b, dates in special.items():\n",
    "            sel = df[\"building_number\"].eq(b) & df[\"date\"].astype(str).isin(dates)\n",
    "            df.loc[sel, \"holiday\"] = 1\n",
    "\n",
    "        # 격주 규칙\n",
    "        mark_nth_weekday(27, 6, {2, 4})  # 27: 2·4번째 일요일\n",
    "        mark_nth_weekday(29, 6, {4})     # 29: 4번째 일요일\n",
    "        mark_nth_weekday(32, 0, {2, 4})  # 32: 2·4번째 월요일\n",
    "        for b in [40, 59, 63]:           # 2·4번째 일요일\n",
    "            mark_nth_weekday(b, 6, {2, 4})\n",
    "\n",
    "        # 34,73,88: 휴일 없음 → holiday=0 유지\n",
    "\n",
    "    # 안전 재확인: IDC 67의 8/15\n",
    "    df.loc[(df[\"building_number\"].eq(67)) & (df[\"date\"].astype(str) == \"2024-08-15\"), \"holiday\"] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    _ensure_dt(df)\n",
    "\n",
    "    rules_lt = [\n",
    "        # (building_number, threshold)\n",
    "        # Apartment\n",
    "        (25, 0, \"eq\"), (70, 200, \"lt\"),\n",
    "        # Hospital\n",
    "        (44, 800, \"lt\"), (90, 800, \"lt\"), (42, 2000, \"lt\"), (17, 1000, \"lt\"),\n",
    "        # Public\n",
    "        (68, 600, \"lt\"), (72, 600, \"lt\"), (80, 600, \"lt\"), (92, 200, \"lt\"),\n",
    "        # Hotel\n",
    "        (98, 500, \"lt\"),\n",
    "        # Other\n",
    "        (97, 500, \"lt\"), (78, 400, \"lt\"), (26, 300, \"lt\"), (7, 2000, \"lt\"),\n",
    "        # Commercial\n",
    "        (76, 2000, \"lt\"), (41, 2200, \"lt\"), (20, 1600, \"lt\"),\n",
    "        # School\n",
    "        (5, 2000, \"lt\"), (8, 250, \"lt\"), (12, 3500, \"lt\"),\n",
    "        # IDC\n",
    "        (67, 7333, \"lt\"), (81, 800, \"lt\"), (52, 2000, \"lt\"), (43, 6000, \"lt\"), (30, 8000, \"lt\"),\n",
    "    ]\n",
    "\n",
    "    # 값 기반 제거\n",
    "    mask_ok = pd.Series(True, index=df.index)\n",
    "    pc = df[\"power_consumption\"]\n",
    "    bnum = df[\"building_number\"]\n",
    "\n",
    "    for bn, th, op in rules_lt:\n",
    "        if op == \"lt\":\n",
    "            mask_ok &= ~((bnum.eq(bn)) & (pc < th))\n",
    "        elif op == \"eq\":\n",
    "            mask_ok &= ~((bnum.eq(bn)) & (pc == th))\n",
    "\n",
    "    # 기간 기반 제거\n",
    "    # Hotel 10: 2024-07-05 ~ 2024-08-22\n",
    "    mask_ok &= ~(\n",
    "        (bnum.eq(10)) &\n",
    "        (df[\"date_time\"].between(pd.Timestamp(\"2024-07-05\"), pd.Timestamp(\"2024-08-22\")))\n",
    "    )\n",
    "    # IDC 57: 2024-06-07 이전\n",
    "    mask_ok &= ~(\n",
    "        (bnum.eq(57)) & (df[\"date_time\"] < pd.Timestamp(\"2024-06-07\"))\n",
    "    )\n",
    "    # Research 94: 2024-07-27 09:00 ~ 2024-08-04 23:00\n",
    "    mask_ok &= ~(\n",
    "        (bnum.eq(94)) &\n",
    "        (df[\"date_time\"].between(pd.Timestamp(\"2024-07-27 09:00\"), pd.Timestamp(\"2024-08-04 23:00\")))\n",
    "    )\n",
    "\n",
    "    return df.loc[mask_ok].reset_index(drop=True)\n",
    "\n",
    "def add_summer_cycle_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    여름 기간(6/1~9/14)을 주기로 하는 sin/cos 특성을 생성합니다.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    start_date = datetime.strptime(\"2024-06-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_date = datetime.strptime(\"2024-09-14 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    period_seconds = (end_date - start_date).total_seconds()\n",
    "    \n",
    "    def summer_cos(date):\n",
    "        return np.cos(2 * np.pi * (date - start_date).total_seconds() / period_seconds)\n",
    "    \n",
    "    def summer_sin(date):\n",
    "        return np.sin(2 * np.pi * (date - start_date).total_seconds() / period_seconds)\n",
    "        \n",
    "    df_copy['summer_cos'] = df_copy['date_time'].apply(summer_cos)\n",
    "    df_copy['summer_sin'] = df_copy['date_time'].apply(summer_sin)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def add_squared_features(\n",
    "    df: pd.DataFrame, \n",
    "    target_cols: List[str] = ['temperature', 'humidity']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    지정된 컬럼에 대해 제곱(squared) 특성을 생성합니다.\n",
    "    변수가 타겟에 미치는 비선형 관계를 모델이 학습하는 데 도움을 줍니다.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 특성을 추가할 데이터프레임\n",
    "        target_cols (List[str]): 제곱할 대상 컬럼 리스트\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 제곱 특성이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in target_cols:\n",
    "        df_copy[f'{col}_squared'] = df_copy[col] ** 2\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def create_cyclic_features(df):\n",
    "    \"\"\"\n",
    "    사이클릭 피처 추가하는 함수 (create_datetime 이후에 사용)\n",
    "    \"\"\"\n",
    "    # hour: 0–23\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    # day_of_week: 0–6\n",
    "    df['sin_dow'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['cos_dow'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # month: 1–12 → 0–11로 변환\n",
    "    df['month0'] = df['month'] - 1\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month0'] / 12)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month0'] / 12)\n",
    "    df.drop(columns=['month0'], inplace=True)\n",
    "    \n",
    "    # (선택) 날짜 전체 주기: day_of_year 1–365 or 366\n",
    "    df['sin_doy'] = np.sin(2 * np.pi * (df['day_of_year'] - 1) / 365)\n",
    "    df['cos_doy'] = np.cos(2 * np.pi * (df['day_of_year'] - 1) / 365)\n",
    "    \n",
    "    # 원본 컬럼 제거\n",
    "    #df.drop(columns=['hour', 'day_of_week', 'month', 'day_of_year'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def CDH(xs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cooling Degree Hours 계산: 기준 26°C 대비 초과 온도의 누적\n",
    "    \"\"\"\n",
    "    cumsum = np.cumsum(xs - 26)\n",
    "    return np.concatenate((cumsum[:11], cumsum[11:] - cumsum[:-11]))\n",
    "\n",
    "\n",
    "def add_cdh_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    건물별 온도 데이터를 이용해 CDH 피처 추가\n",
    "    \"\"\"\n",
    "    cdhs = []\n",
    "    for b in df['building_number'].unique():\n",
    "        temps = df.loc[df['building_number'] == b, 'temperature'].values\n",
    "        cdhs.append(CDH(temps))\n",
    "    df['CDH'] = np.concatenate(cdhs)\n",
    "    return df\n",
    "\n",
    "def add_cdd_feature(df: pd.DataFrame,\n",
    "                    base_temp: float = 18.0,\n",
    "                    window:    int   = 24\n",
    "                   ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cooling Degree Days (CDD) 추가\n",
    "      - base_temp (°C) 보다 높을 때만 (T - base_temp) 합산\n",
    "      - window 시간 롤링 합산 (min_periods=1)\n",
    "    \"\"\"\n",
    "    # 1) per-hour 초과분 계산\n",
    "    df['excess'] = (df['temperature'] - base_temp).clip(lower=0)\n",
    "\n",
    "    # 2) building_number 그룹별 rolling sum\n",
    "    df['CDD'] = (df\n",
    "                 .groupby('building_number')['excess']\n",
    "                 .transform(lambda s: s.rolling(window, min_periods=1).sum())\n",
    "                )\n",
    "\n",
    "    # 3) 중간 컬럼 정리\n",
    "    df.drop(columns=['excess'], inplace=True)\n",
    "    return df\n",
    "def add_thi_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Temperature-Humidity Index (THI) 추가\n",
    "    \"\"\"\n",
    "    df['THI'] = (9/5 * df['temperature'] \n",
    "                 - 0.55 * (1 - df['humidity']/100) \n",
    "                 * (9/5 * df['temperature'] - 26) \n",
    "                 + 32)\n",
    "    return df\n",
    "def add_wct_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Wind Chill Temperature (WCT) 추가\n",
    "    \"\"\"\n",
    "    v16 = df['windspeed'] ** 0.16\n",
    "    df['WCT'] = (13.12 \n",
    "                 + 0.6125 * df['temperature'] \n",
    "                 - 11.37 * v16 \n",
    "                 + 0.3965 * v16 * df['temperature'])\n",
    "    return df\n",
    "\n",
    "def add_temp_features(data):\n",
    "    avg_temp = (\n",
    "        pd.pivot_table(\n",
    "            data[data['hour'] % 3 == 0],\n",
    "            values='temperature',\n",
    "            index=['building_number', 'day', 'month'],\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={'temperature': 'avg_temp'})\n",
    "    )\n",
    "    data = pd.merge(data, avg_temp, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    max_temp = (\n",
    "        pd.pivot_table(\n",
    "            data,\n",
    "            values='temperature',\n",
    "            index=['building_number', 'day', 'month'],\n",
    "            aggfunc='max'\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={'temperature': 'max_temp'})\n",
    "    )\n",
    "    data = pd.merge(data, max_temp, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    min_temp = (\n",
    "        pd.pivot_table(\n",
    "            data,\n",
    "            values='temperature',\n",
    "            index=['building_number', 'day', 'month'],\n",
    "            aggfunc='min'\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={'temperature': 'min_temp'})\n",
    "    )\n",
    "    data = pd.merge(data, min_temp, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    data['temp_diff'] = data['max_temp'] - data['min_temp']\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _prep(df, time_col, group_col):\n",
    "    \"\"\"정렬 헬퍼\"\"\"\n",
    "    return df.sort_values([group_col, time_col])\n",
    "\n",
    "\n",
    "\n",
    "def add_weekly_slope(df: pd.DataFrame,\n",
    "                     time_col: str = 'date_time',\n",
    "                     group_col: str = 'building_number',\n",
    "                     power_col: str = 'power_consumption',\n",
    "                     lookback: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1주일 전 최근 lookback 시간의 선형회귀 기울기(β) 피처 추가\n",
    "    \"\"\"\n",
    "    df = _prep(df, time_col, group_col)\n",
    "\n",
    "    def _beta(x: pd.Series) -> float:\n",
    "        if x.isna().any(): return np.nan\n",
    "        idx = np.arange(len(x))\n",
    "        num = idx.dot(x) * len(x) - idx.sum() * x.sum()\n",
    "        den = len(x) * (idx**2).sum() - idx.sum()**2\n",
    "        return num / den if den else 0.0\n",
    "\n",
    "    pw_seq = df.groupby(group_col)[power_col].shift(WEEK_H)\n",
    "    col = f'power_week_slope{lookback}h'\n",
    "    df[col] = pw_seq.groupby(df[group_col]).transform(\n",
    "        lambda s: s.rolling(lookback).apply(_beta, raw=False)\n",
    "    ).fillna(0)\n",
    "    return df\n",
    "\n",
    "def mean_std_power(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    is_train = df['power_consumption'].notna()\n",
    "    \n",
    "    # date, hour, day_of_week 컬럼 준비\n",
    "    if 'date' not in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date_time']).dt.date\n",
    "    if 'hour' not in df.columns:\n",
    "        df['hour'] = pd.to_datetime(df['date_time']).dt.hour\n",
    "    if 'day_of_week' not in df.columns:\n",
    "        df['day_of_week'] = pd.to_datetime(df['date_time']).dt.weekday\n",
    "\n",
    "    # 1) 기존에 만들어둔 holiday 플래그 사용\n",
    "    #    (0/1로 되어 있다고 가정)\n",
    "    df['holiday'] = df['holiday'].fillna(0).astype(int)\n",
    "\n",
    "    # 2) (선택) 학습 데이터에만 power 조정이 필요하면 ratio 적용\n",
    "    base_ratio = np.array([0.985] + [0.98]*2 + [0.995]*2 + [0.99]*2)\n",
    "    ratio_all = base_ratio - 0.005\n",
    "    df.loc[is_train, 'power_consumption'] = df.loc[is_train].apply(\n",
    "        lambda r: r['power_consumption'] * ratio_all[int(r['day_of_week'])],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    train_df = df[is_train]\n",
    "\n",
    "    # 3-A) 요일·시간별 평균\n",
    "    dow_hour = (\n",
    "        train_df\n",
    "        .groupby(['building_number', 'hour', 'day_of_week'])['power_consumption']\n",
    "        .mean()\n",
    "        .reset_index(name='dow_hour_mean')\n",
    "    )\n",
    "    df = df.merge(dow_hour, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "\n",
    "    # 3-B) holiday_mean & holiday_std (holiday 기준)\n",
    "    hol_mean = (\n",
    "        train_df\n",
    "        .groupby(['building_number', 'hour', 'holiday'])['power_consumption']\n",
    "        .mean()\n",
    "        .reset_index(name='holiday_mean')\n",
    "    )\n",
    "    hol_std = (\n",
    "        train_df\n",
    "        .groupby(['building_number', 'hour', 'holiday'])['power_consumption']\n",
    "        .std()\n",
    "        .reset_index(name='holiday_std')\n",
    "    )\n",
    "    df = df.merge(hol_mean, on=['building_number', 'hour', 'holiday'], how='left')\n",
    "    df = df.merge(hol_std,  on=['building_number', 'hour', 'holiday'], how='left')\n",
    "\n",
    "    # 3-C) 시간(hour)별 평균·표준편차\n",
    "    hr_mean = (\n",
    "        train_df\n",
    "        .groupby(['building_number', 'hour'])['power_consumption']\n",
    "        .mean()\n",
    "        .reset_index(name='hour_mean')\n",
    "    )\n",
    "    hr_std = (\n",
    "        train_df\n",
    "        .groupby(['building_number', 'hour'])['power_consumption']\n",
    "        .std()\n",
    "        .reset_index(name='hour_std')\n",
    "    )\n",
    "    df = df.merge(hr_mean, on=['building_number', 'hour'], how='left')\n",
    "    df = df.merge(hr_std,  on=['building_number', 'hour'], how='left')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04ca050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS = ['sunshine','solar_radiation','date_time', 'solar_power_capacity','ess_capacity', 'pcs_capacity',\n",
    "             'hour', 'day_of_week', 'month', 'day_of_year',]\n",
    "CAT_COLS = ['building_type', 'building_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd21c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to: ../data/processed/train.pkl\n",
      "Saved processed data to: ../data/processed/test.pkl\n"
     ]
    }
   ],
   "source": [
    "train, test = load_and_process(\"./data\")\n",
    "train, test = create_datetime(train), create_datetime(test)\n",
    "combined_df = pd.concat([train, test], ignore_index=True)\n",
    "combined_df = add_holiday(combined_df) \n",
    "combined_df = remove_outliers(combined_df)\n",
    "combined_df = add_squared_features(combined_df)\n",
    "combined_df = add_summer_cycle_features(combined_df)\n",
    "combined_df = create_cyclic_features(combined_df)\n",
    "comgined_df = add_cdh_feature(combined_df)\n",
    "combined_df = add_cdd_feature(combined_df)\n",
    "combined_df = add_thi_feature(combined_df)\n",
    "combined_df = add_wct_feature(combined_df)\n",
    "combined_df = add_temp_features(combined_df)\n",
    "combined_df = mean_std_power(combined_df)\n",
    "combined_df = add_weekly_slope(combined_df)\n",
    "split_date = pd.to_datetime('2024-08-25 00:00:00')\n",
    "val_date   = split_date - pd.Timedelta(days=7)\n",
    "train = combined_df[combined_df['date_time'] < split_date].copy()\n",
    "test  = combined_df[combined_df['date_time'] >= split_date].copy()\n",
    "\n",
    "for c in CAT_COLS:\n",
    "    train[c] = train[c].astype('category')\n",
    "    test[c]  = test[c].astype('category')\n",
    "# 3. 피처/타겟 분리\n",
    "train_cols_drop = ['date_time','date'] + DROP_COLS\n",
    "test_cols_drop = ['date_time','power_consumption','date'] + DROP_COLS\n",
    "train    = train.drop(columns=train_cols_drop, errors='ignore')\n",
    "test  = test.drop(columns=test_cols_drop, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ce97973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_number', 'temperature', 'rainfall', 'windspeed', 'humidity',\n",
       "       'power_consumption', 'building_type', 'total_area', 'cooling_area',\n",
       "       'day', 'is_weekend', 'time_of_day', 'weekday', 'holiday',\n",
       "       'temperature_squared', 'humidity_squared', 'summer_cos', 'summer_sin',\n",
       "       'sin_hour', 'cos_hour', 'sin_dow', 'cos_dow', 'sin_month', 'cos_month',\n",
       "       'sin_doy', 'cos_doy', 'CDH', 'CDD', 'THI', 'WCT', 'avg_temp',\n",
       "       'max_temp', 'min_temp', 'temp_diff', 'dow_hour_mean', 'holiday_mean',\n",
       "       'holiday_std', 'hour_mean', 'hour_std', 'power_week_slope6h'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b3fab0",
   "metadata": {},
   "source": [
    "## Train & Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16c0026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.core.metrics import make_scorer\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49fe66a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250827_122935\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.3\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2\n",
      "CPU Count:          32\n",
      "Memory Avail:       47.65 GB / 62.72 GB (76.0%)\n",
      "Disk Space Avail:   249.62 GB / 937.82 GB (26.6%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=5, num_bag_sets=3\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "2025-08-27 21:29:36,065\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-08-27 21:29:37,151\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/kiwoong/workspace/ZEUP_FINAL/AutogluonModels/ag-20250827_122935/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Beginning AutoGluon training ... Time limit = 898s\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m AutoGluon will save models to \"/home/kiwoong/workspace/ZEUP_FINAL/AutogluonModels/ag-20250827_122935/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Train Data Rows:    179304\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Train Data Columns: 39\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Label Column:       power_consumption\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tAvailable Memory:                    46824.16 MB\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tTrain Data (Original)  Memory Usage: 49.59 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\t('category', []) :  2 | ['building_number', 'building_type']\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\t('float', [])    : 33 | ['temperature', 'rainfall', 'windspeed', 'humidity', 'total_area', ...]\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\t('int', [])      :  4 | ['day', 'is_weekend', 'weekday', 'holiday']\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\t('category', [])  :  2 | ['building_number', 'building_type']\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\t('float', [])     : 33 | ['temperature', 'rainfall', 'windspeed', 'humidity', 'total_area', ...]\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\t('int', [])       :  2 | ['day', 'weekday']\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t\t('int', ['bool']) :  2 | ['is_weekend', 'holiday']\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t0.4s = Fit runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t39 features in original data used to generate 39 features in processed data.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tTrain Data (Processed) Memory Usage: 47.20 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'symmetric_mean_absolute_percentage_error'\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 598.26s of the 897.61s of remaining time.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t-0.0329\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t0.14s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t10.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 587.65s of the 887.00s of remaining time.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t-0.0316\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t0.13s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t10.35s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 576.93s of the 876.28s of remaining time.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (15 workers, per: cpus=2, gpus=0, memory=0.65%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3452105)\u001b[0m [1000]\tvalid_set's l2: 30837.3\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0254997\n",
      "\u001b[36m(_ray_fit pid=3452111)\u001b[0m [1000]\tvalid_set's l2: 31040\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0262569\u001b[32m [repeated 12x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452105)\u001b[0m [2000]\tvalid_set's l2: 25750.1\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0232019\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452112)\u001b[0m [2000]\tvalid_set's l2: 25859.6\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0234018\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452111)\u001b[0m [2000]\tvalid_set's l2: 25702.4\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0238695\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452108)\u001b[0m [3000]\tvalid_set's l2: 22425.9\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0216709\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452110)\u001b[0m [3000]\tvalid_set's l2: 22597.3\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0225229\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452105)\u001b[0m [4000]\tvalid_set's l2: 22058.9\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0209115\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452104)\u001b[0m [4000]\tvalid_set's l2: 21185.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0208727\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452112)\u001b[0m [4000]\tvalid_set's l2: 21915.3\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0213263\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452099)\u001b[0m [5000]\tvalid_set's l2: 19634.9\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0200935\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452108)\u001b[0m [5000]\tvalid_set's l2: 19877.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0201425\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452102)\u001b[0m [5000]\tvalid_set's l2: 20799.9\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0205404\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452100)\u001b[0m [5000]\tvalid_set's l2: 22183.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0201706\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452109)\u001b[0m [6000]\tvalid_set's l2: 20418.1\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0196259\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452098)\u001b[0m [5000]\tvalid_set's l2: 19256.8\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0203028\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452108)\u001b[0m [7000]\tvalid_set's l2: 18542.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0193165\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452109)\u001b[0m [7000]\tvalid_set's l2: 19863.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0192407\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452111)\u001b[0m [6000]\tvalid_set's l2: 19551.2\tvalid_set's symmetric_mean_absolute_percentage_error: -0.020264\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452110)\u001b[0m [8000]\tvalid_set's l2: 18260.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0192485\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452109)\u001b[0m [8000]\tvalid_set's l2: 19394\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0189083\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452099)\u001b[0m [9000]\tvalid_set's l2: 17484.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0185171\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452098)\u001b[0m [7000]\tvalid_set's l2: 17984.8\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0193583\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452099)\u001b[0m [10000]\tvalid_set's l2: 17185.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0183104\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452111)\u001b[0m [8000]\tvalid_set's l2: 18420.4\tvalid_set's symmetric_mean_absolute_percentage_error: -0.019464\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452100)\u001b[0m [9000]\tvalid_set's l2: 20060.2\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0186833\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452111)\u001b[0m [9000]\tvalid_set's l2: 18020.2\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0191415\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3452098)\u001b[0m [9000]\tvalid_set's l2: 17137.3\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0187639\n",
      "\u001b[36m(_ray_fit pid=3452100)\u001b[0m [10000]\tvalid_set's l2: 19739.2\tvalid_set's symmetric_mean_absolute_percentage_error: -0.018453\n",
      "\u001b[36m(_ray_fit pid=3452111)\u001b[0m [10000]\tvalid_set's l2: 17675.3\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0187994\n",
      "\u001b[36m(_ray_fit pid=3452098)\u001b[0m [10000]\tvalid_set's l2: 16817.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0184703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t-0.0175\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t217.8s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t1804.39s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 263.94s of the 563.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (15 workers, per: cpus=2, gpus=0, memory=0.66%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3453323)\u001b[0m [1000]\tvalid_set's l2: 26388.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.024148\n",
      "\u001b[36m(_ray_fit pid=3453326)\u001b[0m [1000]\tvalid_set's l2: 27357.9\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0241296\n",
      "\u001b[36m(_ray_fit pid=3453335)\u001b[0m [1000]\tvalid_set's l2: 26119.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0238702\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453323)\u001b[0m [2000]\tvalid_set's l2: 22329.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0216892\n",
      "\u001b[36m(_ray_fit pid=3453331)\u001b[0m [2000]\tvalid_set's l2: 22496.3\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0211712\n",
      "\u001b[36m(_ray_fit pid=3453330)\u001b[0m [2000]\tvalid_set's l2: 21704.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0213918\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453323)\u001b[0m [3000]\tvalid_set's l2: 20477.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0204942\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453328)\u001b[0m [3000]\tvalid_set's l2: 19704.2\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0204744\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453335)\u001b[0m [3000]\tvalid_set's l2: 20152.2\tvalid_set's symmetric_mean_absolute_percentage_error: -0.020437\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453331)\u001b[0m [4000]\tvalid_set's l2: 19726.1\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0192089\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453333)\u001b[0m [4000]\tvalid_set's l2: 18943.3\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0196729\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453325)\u001b[0m [5000]\tvalid_set's l2: 20106.6\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0191273\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453321)\u001b[0m [5000]\tvalid_set's l2: 17667.8\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0188685\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453324)\u001b[0m [5000]\tvalid_set's l2: 17732\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0191536\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453330)\u001b[0m [5000]\tvalid_set's l2: 18310.3\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0190787\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453322)\u001b[0m [6000]\tvalid_set's l2: 16882\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0186088\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453334)\u001b[0m [6000]\tvalid_set's l2: 18325.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0188791\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453323)\u001b[0m [7000]\tvalid_set's l2: 17987.9\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0184665\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453326)\u001b[0m [7000]\tvalid_set's l2: 18831.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0185456\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453334)\u001b[0m [7000]\tvalid_set's l2: 17982.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0185579\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453332)\u001b[0m [8000]\tvalid_set's l2: 16083.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0182024\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453325)\u001b[0m [9000]\tvalid_set's l2: 18853.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0179049\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453334)\u001b[0m [8000]\tvalid_set's l2: 17699.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0182223\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453326)\u001b[0m [9000]\tvalid_set's l2: 18421.6\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0181079\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453325)\u001b[0m [10000]\tvalid_set's l2: 18673.6\tvalid_set's symmetric_mean_absolute_percentage_error: -0.017705\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453332)\u001b[0m [10000]\tvalid_set's l2: 15747.2\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0178446\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453333)\u001b[0m [9000]\tvalid_set's l2: 17096.2\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0178281\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453324)\u001b[0m [10000]\tvalid_set's l2: 16260.2\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0177238\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3453333)\u001b[0m [10000]\tvalid_set's l2: 16954.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0176508\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t-0.0167\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t196.7s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t1560.32s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 264.84s of remaining time.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 0.667, 'LightGBMXT_BAG_L1': 0.333}\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t-0.0164\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t0.09s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 264.73s of the 264.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (15 workers, per: cpus=2, gpus=0, memory=0.69%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3454554)\u001b[0m [1000]\tvalid_set's l2: 20066.4\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0193441\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454551)\u001b[0m [1000]\tvalid_set's l2: 19097\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0193577\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454554)\u001b[0m [2000]\tvalid_set's l2: 18442\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0179204\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454555)\u001b[0m [2000]\tvalid_set's l2: 19297.9\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0180451\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454547)\u001b[0m [2000]\tvalid_set's l2: 18330.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0182415\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454556)\u001b[0m [3000]\tvalid_set's l2: 17705.6\tvalid_set's symmetric_mean_absolute_percentage_error: -0.017107\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454546)\u001b[0m [3000]\tvalid_set's l2: 17294\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0172912\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454548)\u001b[0m [3000]\tvalid_set's l2: 19231.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0178756\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454559)\u001b[0m [3000]\tvalid_set's l2: 18465.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0177032\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454546)\u001b[0m [4000]\tvalid_set's l2: 17035.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.016999\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454548)\u001b[0m [4000]\tvalid_set's l2: 19038.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0175606\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454559)\u001b[0m [4000]\tvalid_set's l2: 18194.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0173682\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454551)\u001b[0m [5000]\tvalid_set's l2: 17076.8\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0169226\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454549)\u001b[0m [5000]\tvalid_set's l2: 17835.6\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0170429\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454559)\u001b[0m [5000]\tvalid_set's l2: 18077.5\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0170166\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454552)\u001b[0m [6000]\tvalid_set's l2: 16239.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0165288\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454556)\u001b[0m [7000]\tvalid_set's l2: 17193.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.016451\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454547)\u001b[0m [6000]\tvalid_set's l2: 17611.1\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0168217\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454552)\u001b[0m [7000]\tvalid_set's l2: 16197.9\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0163968\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454558)\u001b[0m [7000]\tvalid_set's l2: 16603\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0164975\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454547)\u001b[0m [7000]\tvalid_set's l2: 17602\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0167332\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454553)\u001b[0m [8000]\tvalid_set's l2: 18237.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0168929\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454549)\u001b[0m [8000]\tvalid_set's l2: 17709.9\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0166453\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454547)\u001b[0m [8000]\tvalid_set's l2: 17572.3\tvalid_set's symmetric_mean_absolute_percentage_error: -0.01663\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454550)\u001b[0m [9000]\tvalid_set's l2: 16951.8\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0163843\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454549)\u001b[0m [9000]\tvalid_set's l2: 17683.3\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0165661\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454548)\u001b[0m [9000]\tvalid_set's l2: 18705.7\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0165645\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454560)\u001b[0m [9000]\tvalid_set's l2: 16684.4\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0167279\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454558)\u001b[0m [10000]\tvalid_set's l2: 16525.2\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0162774\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3454557)\u001b[0m [10000]\tvalid_set's l2: 18287.4\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0164036\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t-0.0158\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t204.62s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t1916.37s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -60.76s of remaining time.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.727, 'LightGBM_BAG_L1': 0.273}\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t-0.0156\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t0.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m AutoGluon training complete, total runtime = 959.02s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 6.8 rows/s (35861 batch size)\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t0.14s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t10.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t0.13s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t10.35s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t25.37s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t25.0s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 0.667, 'LightGBMXT_BAG_L1': 0.333}\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t0.09s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t27.32s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.727, 'LightGBM_BAG_L1': 0.273}\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m \t0.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Refit complete, total runtime = 81.65s ... Best model: \"WeightedEnsemble_L3\"\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/kiwoong/workspace/ZEUP_FINAL/AutogluonModels/ag-20250827_122935/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=3449684)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                         model  score_holdout  score_val                               eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0            LightGBMXT_BAG_L2      -0.014574  -0.015777  symmetric_mean_absolute_percentage_error       73.168613    5301.657048  619.391528                22.234416             1916.373674         204.620824            2       True          6\n",
      "1          WeightedEnsemble_L3      -0.014648  -0.015607  symmetric_mean_absolute_percentage_error       73.170074    5301.658701  619.490984                 0.001461                0.001654           0.099456            3       True          7\n",
      "2     WeightedEnsemble_L3_FULL      -0.015368        NaN  symmetric_mean_absolute_percentage_error       11.238679            NaN   78.053926                 0.001482                     NaN           0.099456            3       True         14\n",
      "3       LightGBMXT_BAG_L2_FULL      -0.015554        NaN  symmetric_mean_absolute_percentage_error       11.237197            NaN   77.954469                 1.413145                     NaN          27.318232            2       True         13\n",
      "4          WeightedEnsemble_L2      -0.015994  -0.016441  symmetric_mean_absolute_percentage_error       43.890828    3364.714754  414.596820                 0.001490                0.001832           0.092066            2       True          5\n",
      "5              LightGBM_BAG_L1      -0.016052  -0.016675  symmetric_mean_absolute_percentage_error       20.591546    1560.323097  196.700353                20.591546             1560.323097         196.700353            1       True          4\n",
      "6     WeightedEnsemble_L2_FULL      -0.016528        NaN  symmetric_mean_absolute_percentage_error        2.874318            NaN   50.462353                 0.001451                     NaN           0.092066            2       True         12\n",
      "7            LightGBMXT_BAG_L1      -0.016889  -0.017506  symmetric_mean_absolute_percentage_error       23.297792    1804.389824  217.804401                23.297792             1804.389824         217.804401            1       True          3\n",
      "8         LightGBM_BAG_L1_FULL      -0.017063        NaN  symmetric_mean_absolute_percentage_error        1.346488            NaN   25.002470                 1.346488                     NaN          25.002470            1       True         11\n",
      "9       LightGBMXT_BAG_L1_FULL      -0.017861        NaN  symmetric_mean_absolute_percentage_error        1.526378            NaN   25.367817                 1.526378                     NaN          25.367817            1       True         10\n",
      "10  KNeighborsDist_BAG_L1_FULL      -0.031406        NaN  symmetric_mean_absolute_percentage_error        3.496777      10.346795    0.130869                 3.496777               10.346795           0.130869            1       True          9\n",
      "11       KNeighborsDist_BAG_L1      -0.031406  -0.031580  symmetric_mean_absolute_percentage_error        3.568855      10.346795    0.130869                 3.568855               10.346795           0.130869            1       True          2\n",
      "12  KNeighborsUnif_BAG_L1_FULL      -0.032704        NaN  symmetric_mean_absolute_percentage_error        3.454409      10.223658    0.135081                 3.454409               10.223658           0.135081            1       True          8\n",
      "13       KNeighborsUnif_BAG_L1      -0.032704  -0.032935  symmetric_mean_absolute_percentage_error        3.476005      10.223658    0.135081                 3.476005               10.223658           0.135081            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1128s\t = DyStack   runtime |\t2472s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2472s\n",
      "AutoGluon will save models to \"/home/kiwoong/workspace/ZEUP_FINAL/AutogluonModels/ag-20250827_122935\"\n",
      "Train Data Rows:    201717\n",
      "Train Data Columns: 39\n",
      "Label Column:       power_consumption\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    47827.43 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.79 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  2 | ['building_number', 'building_type']\n",
      "\t\t('float', [])    : 33 | ['temperature', 'rainfall', 'windspeed', 'humidity', 'total_area', ...]\n",
      "\t\t('int', [])      :  4 | ['day', 'is_weekend', 'weekday', 'holiday']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  2 | ['building_number', 'building_type']\n",
      "\t\t('float', [])     : 33 | ['temperature', 'rainfall', 'windspeed', 'humidity', 'total_area', ...]\n",
      "\t\t('int', [])       :  2 | ['day', 'weekday']\n",
      "\t\t('int', ['bool']) :  2 | ['is_weekend', 'holiday']\n",
      "\t0.5s = Fit runtime\n",
      "\t39 features in original data used to generate 39 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 53.10 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'symmetric_mean_absolute_percentage_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1646.96s of the 2471.05s of remaining time.\n",
      "\t-0.0323\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t13.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1633.56s of the 2457.65s of remaining time.\n",
      "\t-0.031\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t17.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1616.19s of the 2440.28s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (15 workers, per: cpus=2, gpus=0, memory=0.73%)\n",
      "\t-0.0173\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t181.12s\t = Training   runtime\n",
      "\t2049.82s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1297.75s of the 2121.85s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (15 workers, per: cpus=2, gpus=0, memory=0.74%)\n",
      "\t-0.0164\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t169.95s\t = Training   runtime\n",
      "\t1916.66s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1004.26s of the 1828.35s of remaining time.\n",
      "\t-0.0203\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t47.93s\t = Training   runtime\n",
      "\t4.9s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 950.57s of the 1774.66s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=16, gpus=0, memory=0.77%)\n",
      "\t-0.0268\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t768.63s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 180.91s of the 1005.00s of remaining time.\n",
      "\t-0.0205\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t14.91s\t = Training   runtime\n",
      "\t4.67s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 160.50s of the 984.59s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=16, gpus=0, memory=1.29%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 104.59s of the 928.68s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=16, gpus=0, memory=1.01%)\n",
      "2025-08-27 22:14:12,880\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-0.0316\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t92.37s\t = Training   runtime\n",
      "\t1.24s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 10.88s of the 834.98s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=16, gpus=0, memory=0.71%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 7.10s of the 831.20s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (15 workers, per: cpus=2, gpus=0, memory=0.80%)\n",
      "\t-0.0712\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t6.05s\t = Training   runtime\n",
      "\t1.73s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 823.62s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.625, 'LightGBMXT_BAG_L1': 0.25, 'RandomForestMSE_BAG_L1': 0.125}\n",
      "\t-0.0161\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 823.34s of the 823.30s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (15 workers, per: cpus=2, gpus=0, memory=0.85%)\n",
      "\t-0.0155\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t178.93s\t = Training   runtime\n",
      "\t1550.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 530.17s of the 530.13s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (15 workers, per: cpus=2, gpus=0, memory=0.86%)\n",
      "\t-0.0155\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t135.29s\t = Training   runtime\n",
      "\t208.31s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 358.42s of the 358.38s of remaining time.\n",
      "\t-0.0156\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t105.98s\t = Training   runtime\n",
      "\t7.51s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 243.99s of the 243.95s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=16, gpus=0, memory=0.87%)\n",
      "\t-0.0189\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t203.03s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 39.89s of the 39.85s of remaining time.\n",
      "\t-0.0153\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t21.77s\t = Training   runtime\n",
      "\t6.68s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 10.51s of the 10.47s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=16, gpus=0, memory=1.45%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 6.77s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.308, 'ExtraTreesMSE_BAG_L2': 0.308, 'LightGBM_BAG_L2': 0.231, 'RandomForestMSE_BAG_L2': 0.154}\n",
      "\t-0.015\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2465.19s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 7.0 rows/s (40344 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.15s\t = Training   runtime\n",
      "\t13.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.15s\t = Training   runtime\n",
      "\t17.06s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "2025-08-27 22:29:30,772\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t29.27s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t29.55s\t = Training   runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t47.93s\t = Training   runtime\n",
      "\t4.9s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t24.98s\t = Training   runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t14.91s\t = Training   runtime\n",
      "\t4.67s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\t0.99s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t1.41s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.625, 'LightGBMXT_BAG_L1': 0.25, 'RandomForestMSE_BAG_L1': 0.125}\n",
      "\t0.25s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t28.17s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t14.84s\t = Training   runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t105.98s\t = Training   runtime\n",
      "\t7.51s\t = Validation runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t6.81s\t = Training   runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t21.77s\t = Training   runtime\n",
      "\t6.68s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.308, 'ExtraTreesMSE_BAG_L2': 0.308, 'LightGBM_BAG_L2': 0.231, 'RandomForestMSE_BAG_L2': 0.154}\n",
      "\t0.3s\t = Training   runtime\n",
      "Refit complete, total runtime = 145.77s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/kiwoong/workspace/ZEUP_FINAL/AutogluonModels/ag-20250827_122935\")\n"
     ]
    }
   ],
   "source": [
    "predictor  = TabularPredictor(label=\"power_consumption\", eval_metric='smape', problem_type=\"regression\").fit(train_data=train,\n",
    "presets=[\"best_quality\"],num_stack_levels=1, num_bag_folds=5, num_bag_sets=3, num_gpus=1,refit_full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "876366de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc58d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('./data/sample_submission.csv')\n",
    "submission_df['answer'] = y_pred.values\n",
    "submission_df.to_csv('autogluon.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogluon (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
